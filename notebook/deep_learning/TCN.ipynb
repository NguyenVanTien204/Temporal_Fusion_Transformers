{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaae8293",
   "metadata": {},
   "source": [
    "# 1. Import Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65be97b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TCN Forecasting Pipeline\n",
    "# Mục tiêu: sử dụng bộ dữ liệu đã được feature engineering để huấn luyện mô hình TCN dự báo doanh số\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c44546",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a990b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_parquet('data/model/train_fe.parquet')\n",
    "test_data = pd.read_parquet('data/model/test_fe.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d4f3a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 830972 entries, 0 to 830971\n",
      "Data columns (total 28 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   Store                    830972 non-null  int64  \n",
      " 1   DayOfWeek                830972 non-null  int32  \n",
      " 2   Sales                    830972 non-null  float64\n",
      " 3   Customers                830972 non-null  int64  \n",
      " 4   Open                     830972 non-null  int64  \n",
      " 5   Promo                    830972 non-null  int64  \n",
      " 6   StateHoliday             830972 non-null  object \n",
      " 7   SchoolHoliday            830972 non-null  int64  \n",
      " 8   StoreType                830972 non-null  object \n",
      " 9   Assortment               830972 non-null  object \n",
      " 10  CompetitionDistance      830972 non-null  float64\n",
      " 11  Promo2                   830972 non-null  int64  \n",
      " 12  CompetitionMissingFlag   830972 non-null  int64  \n",
      " 13  LogSales                 830972 non-null  float64\n",
      " 14  Year                     830972 non-null  int32  \n",
      " 15  Month                    830972 non-null  int32  \n",
      " 16  Day                      830972 non-null  int32  \n",
      " 17  WeekOfYear               830972 non-null  int64  \n",
      " 18  IsWeekend                830972 non-null  int64  \n",
      " 19  IsMonthStart             830972 non-null  int64  \n",
      " 20  IsMonthEnd               830972 non-null  int64  \n",
      " 21  CompetitionMonthsActive  566625 non-null  float64\n",
      " 22  Promo2WeeksActive        414227 non-null  float64\n",
      " 23  PromoIntervalActive      830972 non-null  int64  \n",
      " 24  Lag_1                    830954 non-null  float64\n",
      " 25  Lag_7                    825394 non-null  float64\n",
      " 26  Rolling_Mean_7           825394 non-null  float64\n",
      " 27  Rolling_Std_7            825394 non-null  float64\n",
      "dtypes: float64(9), int32(4), int64(12), object(3)\n",
      "memory usage: 164.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ad04a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1113 entries, 0 to 1112\n",
      "Data columns (total 26 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Store                    1113 non-null   int64  \n",
      " 1   DayOfWeek                1113 non-null   int32  \n",
      " 2   Open                     1113 non-null   int64  \n",
      " 3   Promo                    1113 non-null   int64  \n",
      " 4   StateHoliday             1113 non-null   object \n",
      " 5   SchoolHoliday            1113 non-null   int64  \n",
      " 6   StoreType                1113 non-null   object \n",
      " 7   Assortment               1113 non-null   object \n",
      " 8   CompetitionDistance      1113 non-null   float64\n",
      " 9   Promo2                   1113 non-null   int64  \n",
      " 10  CompetitionMissingFlag   1113 non-null   int64  \n",
      " 11  LogSales                 0 non-null      float64\n",
      " 12  Year                     1113 non-null   int32  \n",
      " 13  Month                    1113 non-null   int32  \n",
      " 14  Day                      1113 non-null   int32  \n",
      " 15  WeekOfYear               1113 non-null   int64  \n",
      " 16  IsWeekend                1113 non-null   int64  \n",
      " 17  IsMonthStart             1113 non-null   int64  \n",
      " 18  IsMonthEnd               1113 non-null   int64  \n",
      " 19  CompetitionMonthsActive  759 non-null    float64\n",
      " 20  Promo2WeeksActive        570 non-null    float64\n",
      " 21  PromoIntervalActive      1113 non-null   int64  \n",
      " 22  Lag_1                    1113 non-null   float64\n",
      " 23  Lag_7                    1113 non-null   float64\n",
      " 24  Rolling_Mean_7           1113 non-null   float64\n",
      " 25  Rolling_Std_7            1113 non-null   float64\n",
      "dtypes: float64(8), int32(4), int64(11), object(3)\n",
      "memory usage: 208.8+ KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db856bb",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb2518e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (post-encoding): (830972, 34)\n",
      "Test shape  (post-encoding): (1113, 34)\n",
      "Feature count: 29\n"
     ]
    }
   ],
   "source": [
    "TARGET_COL = \"Sales\"\n",
    "ID_COL = \"Store\"\n",
    "DATE_PARTS = [\"Year\", \"Month\", \"Day\"]\n",
    "CAT_COLS = [\"StateHoliday\", \"StoreType\", \"Assortment\"]\n",
    "LEAK_COLS = {TARGET_COL, \"Customers\", \"LogSales\", \"Date\"}\n",
    "NA_ZERO_COLS = [\n",
    "    \"CompetitionMonthsActive\",\n",
    "    \"Promo2WeeksActive\",\n",
    "    \"Lag_1\",\n",
    "    \"Lag_7\",\n",
    "    \"Rolling_Mean_7\",\n",
    "    \"Rolling_Std_7\",\n",
    "]\n",
    "\n",
    "# Đảm bảo các cột tùy chọn tồn tại\n",
    "for col in [TARGET_COL, \"Customers\", \"LogSales\"]:\n",
    "    if col not in test_data.columns:\n",
    "        test_data[col] = np.nan\n",
    "\n",
    "# Tạo cột Date từ Year/Month/Day\n",
    "for df in [train_data, test_data]:\n",
    "    date_frame = df[DATE_PARTS].rename(columns={\"Year\": \"year\", \"Month\": \"month\", \"Day\": \"day\"})\n",
    "    df[\"Date\"] = pd.to_datetime(date_frame)\n",
    "\n",
    "# Gộp train và test để xử lý đồng nhất\n",
    "train_data[\"dataset\"] = \"train\"\n",
    "test_data[\"dataset\"] = \"test\"\n",
    "combined = pd.concat([train_data, test_data], ignore_index=True)\n",
    "\n",
    "# Xử lý categorical và missing values\n",
    "for col in CAT_COLS:\n",
    "    combined[col] = combined[col].astype(str)\n",
    "\n",
    "for col in NA_ZERO_COLS:\n",
    "    if col in combined.columns:\n",
    "        combined[col] = combined[col].fillna(0)\n",
    "\n",
    "# One-hot encoding\n",
    "combined = pd.get_dummies(combined, columns=CAT_COLS, drop_first=True)\n",
    "combined = combined.sort_values([ID_COL, \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "# Tách lại train và test\n",
    "train_data = combined[combined[\"dataset\"] == \"train\"].drop(columns=[\"dataset\"]).reset_index(drop=True)\n",
    "test_data = combined[combined[\"dataset\"] == \"test\"].drop(columns=[\"dataset\"]).reset_index(drop=True)\n",
    "\n",
    "FEATURE_COLS = [col for col in train_data.columns if col not in LEAK_COLS and col != ID_COL]\n",
    "\n",
    "print(f\"Train shape (post-encoding): {train_data.shape}\")\n",
    "print(f\"Test shape  (post-encoding): {test_data.shape}\")\n",
    "print(f\"Feature count: {len(FEATURE_COLS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ad3c0",
   "metadata": {},
   "source": [
    "# 4. Train/Validation Split & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3594f284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split date: 2015-06-05\n",
      "Train rows: 789,557 | Val rows: 41,415\n"
     ]
    }
   ],
   "source": [
    "VAL_WEEKS = 6\n",
    "seq_horizon = pd.Timedelta(weeks=VAL_WEEKS)\n",
    "split_date = train_data[\"Date\"].max() - seq_horizon\n",
    "\n",
    "train_main = train_data[train_data[\"Date\"] < split_date].copy()\n",
    "val_main = train_data[train_data[\"Date\"] >= split_date].copy()\n",
    "\n",
    "# Fit scalers trên train set\n",
    "feature_scaler = StandardScaler()\n",
    "target_scaler = StandardScaler()\n",
    "\n",
    "train_main[FEATURE_COLS] = feature_scaler.fit_transform(train_main[FEATURE_COLS])\n",
    "val_main[FEATURE_COLS] = feature_scaler.transform(val_main[FEATURE_COLS])\n",
    "test_scaled = test_data.copy()\n",
    "test_scaled[FEATURE_COLS] = feature_scaler.transform(test_scaled[FEATURE_COLS])\n",
    "\n",
    "train_main[[TARGET_COL]] = target_scaler.fit_transform(train_main[[TARGET_COL]])\n",
    "val_main[[TARGET_COL]] = target_scaler.transform(val_main[[TARGET_COL]])\n",
    "\n",
    "print(f\"Split date: {split_date.date()}\")\n",
    "print(f\"Train rows: {len(train_main):,} | Val rows: {len(val_main):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba18e71",
   "metadata": {},
   "source": [
    "# 5. Build Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce18f16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sequences: (756107, 30, 29)\n",
      "Val sequences  : (7977, 30, 29)\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = 30  # days\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "def build_sequences(df: pd.DataFrame, feature_cols, target_col):\n",
    "    \"\"\"Tạo sequences cho time series forecasting\"\"\"\n",
    "    sequences, targets = [], []\n",
    "    for _, group in df.groupby(ID_COL):\n",
    "        group = group.sort_values(\"Date\")\n",
    "        values = group[feature_cols + [target_col]].to_numpy()\n",
    "        if len(values) <= SEQ_LEN:\n",
    "            continue\n",
    "        for start in range(len(values) - SEQ_LEN):\n",
    "            seq_x = values[start:start + SEQ_LEN, :-1]\n",
    "            seq_y = values[start + SEQ_LEN, -1]\n",
    "            sequences.append(seq_x)\n",
    "            targets.append(seq_y)\n",
    "    return np.array(sequences, dtype=np.float32), np.array(targets, dtype=np.float32)\n",
    "\n",
    "X_train, y_train = build_sequences(train_main, FEATURE_COLS, TARGET_COL)\n",
    "X_val, y_val = build_sequences(val_main, FEATURE_COLS, TARGET_COL)\n",
    "\n",
    "print(f\"Train sequences: {X_train.shape}\")\n",
    "print(f\"Val sequences  : {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53ea2da",
   "metadata": {},
   "source": [
    "# 6. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50243e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches -> train: 2954, val: 32\n"
     ]
    }
   ],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32) if y is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = SequenceDataset(X_train, y_train)\n",
    "val_ds = SequenceDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "print(f\"Batches -> train: {len(train_loader)}, val: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69ab863",
   "metadata": {},
   "source": [
    "# 7. TCN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d870200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"Loại bỏ padding thừa ở cuối để đảm bảo causal convolution\"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"Temporal Block là thành phần cơ bản của TCN\"\"\"\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "\n",
    "        # Dilated causal convolution\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "\n",
    "        # Residual connection\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    \"\"\"TCN Network gồm nhiều Temporal Blocks xếp chồng\"\"\"\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i  # Exponentially increasing dilation\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1,\n",
    "                                    dilation=dilation_size,\n",
    "                                    padding=(kernel_size-1) * dilation_size,\n",
    "                                    dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class SalesTCN(nn.Module):\n",
    "    \"\"\"TCN model cho Sales forecasting\"\"\"\n",
    "    def __init__(self, input_dim, num_channels, kernel_size=3, dropout=0.2):\n",
    "        super(SalesTCN, self).__init__()\n",
    "\n",
    "        # TCN expects (batch, channels, seq_len)\n",
    "        # Input sẽ là (batch, seq_len, input_dim)\n",
    "        # Cần transpose\n",
    "\n",
    "        self.tcn = TemporalConvNet(input_dim, num_channels, kernel_size, dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_dim)\n",
    "        # Transpose để phù hợp với Conv1d: (batch, input_dim, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # TCN forward\n",
    "        y = self.tcn(x)\n",
    "\n",
    "        # Lấy output của timestep cuối cùng\n",
    "        y = y[:, :, -1]\n",
    "\n",
    "        # Fully connected layer\n",
    "        out = self.linear(self.dropout(y))\n",
    "        return out.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26663748",
   "metadata": {},
   "source": [
    "# 8. Model Configuration & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "142690d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device: cpu\n",
      "\n",
      "Model Architecture:\n",
      "SalesTCN(\n",
      "  (tcn): TemporalConvNet(\n",
      "    (network): Sequential(\n",
      "      (0): TemporalBlock(\n",
      "        (conv1): Conv1d(29, 64, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(29, 64, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "          (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(2,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (downsample): Conv1d(29, 64, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (1): TemporalBlock(\n",
      "        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "          (4): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (2): TemporalBlock(\n",
      "        (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "          (4): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (downsample): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (3): TemporalBlock(\n",
      "        (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "        (chomp1): Chomp1d()\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "        (chomp2): Chomp1d()\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        (net): Sequential(\n",
      "          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "          (1): Chomp1d()\n",
      "          (2): ReLU()\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "          (4): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "          (5): Chomp1d()\n",
      "          (6): ReLU()\n",
      "          (7): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "\n",
      "Total parameters: 226,369\n",
      "Trainable parameters: 226,369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(FEATURE_COLS)\n",
    "NUM_CHANNELS = [64, 64, 128, 128]  # Số channels cho mỗi temporal block\n",
    "KERNEL_SIZE = 3\n",
    "DROPOUT = 0.2\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SalesTCN(INPUT_DIM, NUM_CHANNELS, KERNEL_SIZE, DROPOUT).to(DEVICE)\n",
    "print(f\"\\nDevice: {DEVICE}\")\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Đếm số parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc3bced",
   "metadata": {},
   "source": [
    "# 9. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f2816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "PATIENCE = 5\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler (optional but recommended)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "def run_epoch(loader, train_mode=True):\n",
    "    \"\"\"Chạy một epoch training hoặc validation\"\"\"\n",
    "    epoch_loss, epoch_mae = 0.0, 0.0\n",
    "    steps = 0\n",
    "\n",
    "    if train_mode:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for batch in loader:\n",
    "        features, targets = [b.to(DEVICE) for b in batch]\n",
    "\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(train_mode):\n",
    "            preds = model(features)\n",
    "            loss = criterion(preds, targets)\n",
    "            mae = torch.mean(torch.abs(preds - targets))\n",
    "\n",
    "            if train_mode:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_mae += mae.item()\n",
    "        steps += 1\n",
    "\n",
    "    return epoch_loss / steps, epoch_mae / steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a5d81",
   "metadata": {},
   "source": [
    "# 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dd549fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING TCN MODEL\n",
      "======================================================================\n",
      "Epoch 01 | Train MSE 0.1353 | Val MSE 0.0684 | Val MAE 0.1907\n",
      "  → New best model saved (val_loss: 0.0684)\n",
      "Epoch 02 | Train MSE 0.0920 | Val MSE 0.0746 | Val MAE 0.2014\n",
      "Epoch 03 | Train MSE 0.0836 | Val MSE 0.0658 | Val MAE 0.1898\n",
      "  → New best model saved (val_loss: 0.0658)\n",
      "Epoch 04 | Train MSE 0.0785 | Val MSE 0.0675 | Val MAE 0.1951\n",
      "Epoch 05 | Train MSE 0.0756 | Val MSE 0.0732 | Val MAE 0.1995\n",
      "Epoch 06 | Train MSE 0.0733 | Val MSE 0.0733 | Val MAE 0.2004\n",
      "Epoch 07 | Train MSE 0.0713 | Val MSE 0.0674 | Val MAE 0.1934\n",
      "Epoch 08 | Train MSE 0.0667 | Val MSE 0.0659 | Val MAE 0.1922\n",
      "\n",
      "Early stopping triggered after 8 epochs\n",
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETED\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING TCN MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "history = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_mae = run_epoch(train_loader, train_mode=True)\n",
    "    val_loss, val_mae = run_epoch(val_loader, train_mode=False)\n",
    "\n",
    "    history.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_mae\": val_mae\n",
    "    })\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train MSE {train_loss:.4f} | Val MSE {val_loss:.4f} | Val MAE {val_mae:.4f}\")\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping & model checkpoint\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"tcn_best.pt\")\n",
    "        print(f\"  → New best model saved (val_loss: {val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch} epochs\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32ce5e1",
   "metadata": {},
   "source": [
    "# 11. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46a9af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(y_true, y_pred):\n",
    "    \"\"\"Mean Absolute Percentage Error\"\"\"\n",
    "    non_zero_mask = y_true != 0\n",
    "    if np.sum(non_zero_mask) == 0:\n",
    "        return np.nan\n",
    "    y_true_filtered = y_true[non_zero_mask]\n",
    "    y_pred_filtered = y_pred[non_zero_mask]\n",
    "    return np.mean(np.abs((y_true_filtered - y_pred_filtered) / y_true_filtered)) * 100\n",
    "\n",
    "def calculate_rmspe(y_true, y_pred):\n",
    "    \"\"\"Root Mean Squared Percentage Error\"\"\"\n",
    "    non_zero_mask = y_true != 0\n",
    "    if np.sum(non_zero_mask) == 0:\n",
    "        return np.nan\n",
    "    y_true_filtered = y_true[non_zero_mask]\n",
    "    y_pred_filtered = y_pred[non_zero_mask]\n",
    "    return np.sqrt(np.mean(((y_true_filtered - y_pred_filtered) / y_true_filtered)**2)) * 100\n",
    "\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    ratio = np.where(denominator == 0, 0, np.abs(y_pred - y_true) / denominator)\n",
    "    return np.mean(ratio) * 100\n",
    "\n",
    "def calculate_mase(y_true, y_pred, naive_forecast_error):\n",
    "    \"\"\"Mean Absolute Scaled Error\"\"\"\n",
    "    if np.isnan(naive_forecast_error) or naive_forecast_error == 0:\n",
    "        return np.nan\n",
    "    mae_val = mean_absolute_error(y_true, y_pred)\n",
    "    return mae_val / naive_forecast_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b42fa",
   "metadata": {},
   "source": [
    "# 12. Final Validation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b860e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "VALIDATION EVALUATION\n",
      "======================================================================\n",
      "✓ Loaded best model weights\n",
      "\n",
      "Validation Metrics:\n",
      "  MAE  : 583.85\n",
      "  RMSE : 790.36\n",
      "  MAPE : 8.38%\n",
      "  RMSPE: 12.71%\n",
      "  sMAPE: 8.12%\n",
      "  MASE : 0.42\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load best model\n",
    "best_path = \"tcn_best.pt\"\n",
    "if os.path.exists(best_path):\n",
    "    model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "    print(\"✓ Loaded best model weights\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Generate predictions\n",
    "all_preds, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for features, targets in val_loader:\n",
    "        features = features.to(DEVICE)\n",
    "        preds = model(features).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_targets.append(targets.numpy())\n",
    "\n",
    "val_preds_scaled = np.concatenate(all_preds)\n",
    "val_targets_scaled = np.concatenate(all_targets)\n",
    "\n",
    "# Inverse transform to original scale\n",
    "val_preds = target_scaler.inverse_transform(val_preds_scaled.reshape(-1, 1)).ravel()\n",
    "val_targets = target_scaler.inverse_transform(val_targets_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(val_targets, val_preds)\n",
    "rmse = np.sqrt(mean_squared_error(val_targets, val_preds))\n",
    "mape = calculate_mape(val_targets, val_preds)\n",
    "rmspe = calculate_rmspe(val_targets, val_preds)\n",
    "smape = calculate_smape(val_targets, val_preds)\n",
    "\n",
    "# MASE calculation\n",
    "original_train_sales = train_data[train_data[\"Date\"] < split_date][TARGET_COL].values\n",
    "if len(original_train_sales) > 1:\n",
    "    naive_forecast_error = np.mean(np.abs(original_train_sales[1:] - original_train_sales[:-1]))\n",
    "    mase = calculate_mase(val_targets, val_preds, naive_forecast_error)\n",
    "else:\n",
    "    mase = np.nan\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nValidation Metrics:\")\n",
    "print(f\"  MAE  : {mae:,.2f}\")\n",
    "print(f\"  RMSE : {rmse:,.2f}\")\n",
    "print(f\"  MAPE : {mape:,.2f}%\")\n",
    "print(f\"  RMSPE: {rmspe:,.2f}%\")\n",
    "print(f\"  sMAPE: {smape:,.2f}%\")\n",
    "print(f\"  MASE : {mase:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d909b79",
   "metadata": {},
   "source": [
    "# 13. Test Set Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "931d1e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST SET INFERENCE\n",
      "======================================================================\n",
      "Test sequences: (1113, 30, 29)\n",
      "\n",
      "✓ Predictions saved to: tcn_predictions.csv\n",
      "  Submission shape: (1113, 3)\n",
      "\n",
      "Sample predictions:\n",
      "   Store ForecastDate  PredictedSales\n",
      "0      1   2015-07-31     3869.942871\n",
      "1      2   2015-07-31     2509.787354\n",
      "2      3   2015-07-31     3952.994141\n",
      "3      4   2015-07-31     9365.440430\n",
      "4      5   2015-07-31     2189.773193\n",
      "5      6   2015-07-31     2566.241943\n",
      "6      7   2015-07-31     6223.457031\n",
      "7      8   2015-07-31     3686.457275\n",
      "8      9   2015-07-31     5929.711914\n",
      "9     10   2015-07-31     4481.862793\n",
      "\n",
      "======================================================================\n",
      "PIPELINE COMPLETED SUCCESSFULLY\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SET INFERENCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def build_test_sequences(test_scaled_df, historical_df, feature_cols, seq_len):\n",
    "    \"\"\"Tạo sequences cho test set từ historical data\"\"\"\n",
    "    sequences, store_ids, forecast_dates = [], [], []\n",
    "\n",
    "    for index, row in test_scaled_df.iterrows():\n",
    "        store_id = row[ID_COL]\n",
    "        forecast_date = row[\"Date\"]\n",
    "        end_date_for_sequence = forecast_date - pd.Timedelta(days=1)\n",
    "\n",
    "        store_history = historical_df[\n",
    "            (historical_df[ID_COL] == store_id) &\n",
    "            (historical_df[\"Date\"] <= end_date_for_sequence)\n",
    "        ].sort_values(\"Date\")\n",
    "\n",
    "        if len(store_history) < seq_len:\n",
    "            continue\n",
    "\n",
    "        seq_features = store_history[feature_cols].tail(seq_len).to_numpy()\n",
    "        sequences.append(seq_features.astype(np.float32))\n",
    "        store_ids.append(store_id)\n",
    "        forecast_dates.append(forecast_date)\n",
    "\n",
    "    return np.array(sequences), store_ids, forecast_dates\n",
    "\n",
    "# Prepare full historical data\n",
    "full_scaled_historical_df = train_data.copy()\n",
    "full_scaled_historical_df[FEATURE_COLS] = feature_scaler.transform(train_data[FEATURE_COLS])\n",
    "full_scaled_historical_df = full_scaled_historical_df.sort_values([ID_COL, \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "# Build test sequences\n",
    "X_test_seq, test_store_ids, forecast_dates = build_test_sequences(\n",
    "    test_scaled,\n",
    "    full_scaled_historical_df,\n",
    "    FEATURE_COLS,\n",
    "    SEQ_LEN\n",
    ")\n",
    "print(f\"Test sequences: {X_test_seq.shape}\")\n",
    "\n",
    "# Create test loader\n",
    "test_ds = SequenceDataset(X_test_seq)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Generate predictions\n",
    "model.eval()\n",
    "test_preds_scaled = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        preds = model(batch.to(DEVICE)).cpu().numpy()\n",
    "        test_preds_scaled.append(preds)\n",
    "\n",
    "test_preds = np.concatenate(test_preds_scaled)\n",
    "test_preds = target_scaler.inverse_transform(test_preds.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    \"Store\": test_store_ids,\n",
    "    \"ForecastDate\": forecast_dates,\n",
    "    \"PredictedSales\": test_preds\n",
    "})\n",
    "\n",
    "output_path = \"tcn_predictions.csv\"\n",
    "submission.to_csv(output_path, index=False)\n",
    "print(f\"\\n✓ Predictions saved to: {output_path}\")\n",
    "print(f\"  Submission shape: {submission.shape}\")\n",
    "print(f\"\\nSample predictions:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb0133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
